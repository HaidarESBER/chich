---
phase: 12-product-sourcing-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified: [src/app/admin/scraper/page.tsx, src/app/admin/scraper/ScraperDashboard.tsx, src/app/admin/scraper/actions.ts, src/app/admin/layout.tsx, src/app/api/cron/scrape/route.ts, vercel.json]
autonomous: true
---

<objective>
Build the admin scraper UI for triggering scrapes and viewing results, and wire the pipeline bridge so scraped products flow into the Phase 13 curation system.

Purpose: Give admins a way to source products by URL and automatically feed them into the AI translation/curation pipeline.
Output: Admin scraper page, server actions for scraping + curation handoff, scheduled scrape cron endpoint.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Scraper engine and data layer from Plan 01:
@.planning/phases/12-product-sourcing-pipeline/12-01-SUMMARY.md

# Phase 13 pipeline bridge function:
@src/lib/pipeline.ts

# Admin layout for sidebar integration:
@src/app/admin/layout.tsx

# Existing admin patterns (curation page):
@src/app/admin/curation/page.tsx
@src/app/admin/curation/CurationDashboard.tsx
@src/app/admin/curation/actions.ts

# Existing cron pattern:
@src/app/api/cron/curate/route.ts
@vercel.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create admin scraper page with URL input and results table</name>
  <files>src/app/admin/scraper/page.tsx, src/app/admin/scraper/ScraperDashboard.tsx, src/app/admin/scraper/actions.ts, src/app/admin/layout.tsx</files>
  <action>
**Server actions** (src/app/admin/scraper/actions.ts):
"use server" module with:
- scrapeUrlAction(formData: FormData): scrapes single URL using scrapeUrl() from engine, returns result or error. Revalidates /admin/scraper.
- scrapeMultipleAction(formData: FormData): takes newline-separated URLs textarea, scrapes sequentially using scrapeUrls() from engine. Revalidates path.
- sendToCurationAction(scrapedProductId: string): calls createDraftFromScrapedProduct() from pipeline.ts with the scraped product data, then calls markSentToCuration() with the draft ID. Revalidates path.
- sendAllToCurationAction(): gets all unsent products (getUnsentProducts()), sends each to curation, returns count. Revalidates path.

**Server component page** (src/app/admin/scraper/page.tsx):
Fetch scraperStats and allScrapedProducts (imported from data.ts). Pass to client component.

**Client component** (src/app/admin/scraper/ScraperDashboard.tsx):
Follow pattern from CurationDashboard.tsx:
- Stats cards row: Total Scraped, Success, Failed, Sent to Curation, Unsent
- URL input section: single URL input + "Scraper" button, or textarea for multiple URLs + "Scraper tout" button. Both use form actions.
- Results table: columns = Image (first thumbnail), Name, Source, Price, Status, Sent, Actions
- Actions column: "Envoyer en curation" button (if not sent), link to source URL
- Bulk action bar: "Envoyer tout en curation" button (sends all unsent)
- Filter tabs: All, Success, Failed, Sent, Unsent
- All UI text in French (existing admin pattern)
- Loading states with useTransition for form actions

**Sidebar link** (src/app/admin/layout.tsx):
Add "Sourcing" link with Search icon from lucide-react, placed between Curation and existing links. Follow same styling pattern.
  </action>
  <verify>npm run build succeeds, /admin/scraper route exists in build output</verify>
  <done>Admin scraper page renders with stats, URL input, results table, and filter tabs. Sidebar has Sourcing link.</done>
</task>

<task type="auto">
  <name>Task 2: Add scheduled scrape cron endpoint and URL list configuration</name>
  <files>src/app/api/cron/scrape/route.ts, vercel.json</files>
  <action>
**Cron endpoint** (src/app/api/cron/scrape/route.ts):
Follow exact pattern from src/app/api/cron/curate/route.ts:
- GET handler with CRON_SECRET verification (header authorization check in production, open in dev)
- Load a predefined URL list from environment variable SCRAPE_URLS (comma-separated) or return early if empty
- Call scrapeUrls() with the URL list
- For each successful scrape that hasn't been sent to curation: call createDraftFromScrapedProduct() and markSentToCuration()
- Return JSON summary: { scraped: N, sentToCuration: N, errors: N, errorDetails: [...] }
- NextResponse with appropriate status

**Vercel config** (vercel.json):
Add a second cron entry alongside the existing curate cron:
```json
{
  "crons": [
    { "path": "/api/cron/curate", "schedule": "0 */6 * * *" },
    { "path": "/api/cron/scrape", "schedule": "0 */12 * * *" }
  ]
}
```
12-hour interval for scraping (less frequent than translation since URLs don't change often).

**Environment:** Add SCRAPE_URLS to .env.local as empty placeholder with comment explaining comma-separated format.
  </action>
  <verify>npm run build succeeds, /api/cron/scrape route exists in build output</verify>
  <done>Cron endpoint scrapes configured URLs and auto-feeds to curation pipeline. vercel.json has both cron schedules.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds
- [ ] /admin/scraper page renders (verify route in build output)
- [ ] /api/cron/scrape endpoint builds
- [ ] Admin sidebar shows Sourcing link
- [ ] vercel.json has both cron entries
</verification>

<success_criteria>
- All tasks completed
- Admin can paste URLs and trigger scrapes
- Scraped products can be sent to curation pipeline individually or in bulk
- Cron endpoint automates scrape + curation handoff
- Full pipeline: URL → scrape → scraped_products → createDraftFromScrapedProduct → product_drafts → AI translation → curation review → publish
</success_criteria>

<output>
After completion, create `.planning/phases/12-product-sourcing-pipeline/12-02-SUMMARY.md`
</output>
