---
phase: 12-product-sourcing-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [supabase/migrations/scraped_products.sql, src/types/scraper.ts, src/lib/scraper/data.ts, src/lib/scraper/engine.ts, src/lib/scraper/adapters/generic.ts, src/lib/scraper/adapters/aliexpress.ts, package.json]
autonomous: true
---

<objective>
Create the scraping infrastructure: database table for raw scraped products, TypeScript types with CRUD data access, and a Cheerio-based scraping engine with source-specific adapters.

Purpose: Provide the data layer and extraction engine that powers the product sourcing pipeline, feeding into Phase 13's curation system via createDraftFromScrapedProduct().
Output: scraped_products table, ScrapedProduct type, CRUD operations, scraping engine with generic + AliExpress adapters.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 13 built the curation pipeline that consumes scraper output:
@src/lib/pipeline.ts
@src/types/curation.ts
@supabase/migrations/product_drafts.sql

# Established patterns for Supabase data access:
@src/lib/curation.ts
@src/lib/supabase/admin.ts

# Existing product types for category reference:
@src/types/product.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scraped_products table, ScrapedProduct type, and CRUD data access</name>
  <files>supabase/migrations/scraped_products.sql, src/types/scraper.ts, src/lib/scraper/data.ts</files>
  <action>
**Database table** (supabase/migrations/scraped_products.sql):
Create `scraped_products` table with columns:
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- source_url TEXT NOT NULL UNIQUE — the URL that was scraped (dedup key)
- source_name TEXT NOT NULL — e.g. 'aliexpress', 'generic', 'wholesaler'
- external_id TEXT — source-specific product ID (e.g. AliExpress item ID)
- raw_name TEXT NOT NULL — extracted product name
- raw_description TEXT — extracted description
- raw_price_text TEXT — price as extracted (string, may include currency symbols)
- raw_images TEXT[] DEFAULT '{}' — extracted image URLs
- raw_category TEXT — extracted category if available
- raw_metadata JSONB DEFAULT '{}' — any extra source-specific data
- scrape_status TEXT NOT NULL DEFAULT 'success' CHECK (scrape_status IN ('success', 'partial', 'failed'))
- error_message TEXT — if scrape failed or was partial
- sent_to_curation BOOLEAN NOT NULL DEFAULT false
- draft_id UUID — references product_drafts(id) if sent to curation, no FK constraint
- created_at TIMESTAMPTZ NOT NULL DEFAULT now()
- updated_at TIMESTAMPTZ NOT NULL DEFAULT now()

Add indexes on source_url (unique), source_name, sent_to_curation.
Enable RLS with admin-only policy (same pattern as product_drafts).
Add update_updated_at trigger.

**TypeScript type** (src/types/scraper.ts):
Define ScrapedProduct interface with camelCase fields matching the table.
Define ScrapeResult type for adapter output: { name, description?, priceText?, images?, category?, metadata?, externalId? }.
Define SourceAdapter interface: { name: string, canHandle(url: string): boolean, extract(html: string, url: string): Promise<ScrapeResult> }.

**Data access** (src/lib/scraper/data.ts):
Follow exact pattern from src/lib/curation.ts — "use server", createAdminClient, toScrapedProduct/toScrapedProductRow mapping helpers.
CRUD functions:
- createScrapedProduct(data: Partial<ScrapedProduct>): Promise<ScrapedProduct>
- getScrapedProductByUrl(url: string): Promise<ScrapedProduct | null> — for dedup check
- getAllScrapedProducts(): Promise<ScrapedProduct[]>
- getScrapedProductsBySource(sourceName: string): Promise<ScrapedProduct[]>
- getUnsentProducts(): Promise<ScrapedProduct[]> — where sent_to_curation = false and scrape_status = 'success'
- updateScrapedProduct(id: string, data: Partial<ScrapedProduct>): Promise<ScrapedProduct>
- markSentToCuration(id: string, draftId: string): Promise<ScrapedProduct> — sets sent_to_curation=true and draft_id
- getScraperStats(): Promise<{total, success, failed, sent, unsent}> — counts for admin dashboard
  </action>
  <verify>TypeScript compiles: npx tsc --noEmit src/types/scraper.ts src/lib/scraper/data.ts (or npm run build)</verify>
  <done>scraped_products.sql migration ready, ScrapedProduct type exported, all CRUD functions compile</done>
</task>

<task type="auto">
  <name>Task 2: Build scraping engine with Cheerio and source adapters</name>
  <files>src/lib/scraper/engine.ts, src/lib/scraper/adapters/generic.ts, src/lib/scraper/adapters/aliexpress.ts, package.json</files>
  <action>
**Install cheerio:** `npm install cheerio` (lightweight HTML parser, no browser needed).

**Generic adapter** (src/lib/scraper/adapters/generic.ts):
Implements SourceAdapter. canHandle() returns true for any URL (fallback adapter).
extract(html, url):
- Use cheerio to load HTML
- Extract name: og:title meta → title tag → first h1
- Extract description: og:description meta → meta description → first paragraph in main/article
- Extract price: look for common patterns — elements with class/id containing "price", currency patterns (€, EUR, USD, $), structured data (JSON-LD Product schema)
- Extract images: og:image meta → product image containers (img tags near price/title), filter out icons/logos (min 200px or no size info)
- Extract category: breadcrumbs if present, or JSON-LD category
- Return ScrapeResult

**AliExpress adapter** (src/lib/scraper/adapters/aliexpress.ts):
Implements SourceAdapter. canHandle(url) returns true if URL contains 'aliexpress.com'.
extract(html, url):
- Extract external_id from URL path (item/{id}.html pattern)
- Extract name from og:title or product-title element
- Extract price from price elements (AliExpress uses specific class patterns for price display)
- Extract images from gallery/image carousel elements
- Extract category from breadcrumb
- NOTE: AliExpress serves dynamic content, so extraction from raw HTML will be limited. This adapter extracts what's available in the initial HTML response + meta tags. For full extraction, a headless browser would be needed (future enhancement).
- Return ScrapeResult with externalId set

**Scraping engine** (src/lib/scraper/engine.ts):
- Import adapters and data access layer
- Register adapters in priority order: [aliexpressAdapter, genericAdapter]
- scrapeUrl(url: string): Promise<ScrapedProduct>:
  1. Check dedup: getScrapedProductByUrl(url). If exists, return existing.
  2. Fetch HTML: use global fetch() with User-Agent header, 10s timeout, handle errors
  3. Find matching adapter: first adapter where canHandle(url) returns true
  4. Extract: adapter.extract(html, url)
  5. Save: createScrapedProduct({ sourceUrl, sourceName: adapter.name, externalId, rawName, rawDescription, rawPriceText, rawImages, rawCategory, rawMetadata, scrapeStatus })
  6. Return saved ScrapedProduct
- scrapeUrls(urls: string[]): Promise<{results: ScrapedProduct[], errors: {url, error}[]}> — sequential with 1s delay between requests (rate limiting)
- Handle fetch errors gracefully: save with scrape_status='failed' and error_message
  </action>
  <verify>npm run build succeeds with no TypeScript errors in scraper modules</verify>
  <done>Cheerio installed, generic and AliExpress adapters extract product data from HTML, engine handles dedup/rate-limiting/errors</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds
- [ ] No TypeScript errors in src/lib/scraper/ or src/types/scraper.ts
- [ ] scraped_products.sql is valid SQL
- [ ] cheerio is in package.json dependencies
</verification>

<success_criteria>
- All tasks completed
- ScrapedProduct type and CRUD match product_drafts pattern
- Scraping engine extracts data from HTML using Cheerio
- Adapters handle AliExpress and generic URLs
- Dedup prevents re-scraping same URL
</success_criteria>

<output>
After completion, create `.planning/phases/12-product-sourcing-pipeline/12-01-SUMMARY.md`
</output>
